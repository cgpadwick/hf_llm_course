{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "895c037e-b466-4b03-ab72-9d5dbe92d18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.github.com/repos/huggingface/datasets/issues?page=1&per_page=1\"\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "222d9b80-b75b-467f-af3f-f49f7feedfe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df6e99f6-91e4-4567-aec4-8bcb73b9c929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://api.github.com/repos/huggingface/datasets/issues/7858',\n",
       "  'repository_url': 'https://api.github.com/repos/huggingface/datasets',\n",
       "  'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/7858/labels{/name}',\n",
       "  'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/7858/comments',\n",
       "  'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/7858/events',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/7858',\n",
       "  'id': 3605471548,\n",
       "  'node_id': 'PR_kwDODunzps6yZq4r',\n",
       "  'number': 7858,\n",
       "  'title': 'Support downloading specific splits in `load_dataset`',\n",
       "  'user': {'login': 'CloseChoice',\n",
       "   'id': 31857876,\n",
       "   'node_id': 'MDQ6VXNlcjMxODU3ODc2',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/31857876?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/CloseChoice',\n",
       "   'html_url': 'https://github.com/CloseChoice',\n",
       "   'followers_url': 'https://api.github.com/users/CloseChoice/followers',\n",
       "   'following_url': 'https://api.github.com/users/CloseChoice/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/CloseChoice/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/CloseChoice/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/CloseChoice/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/CloseChoice/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/CloseChoice/repos',\n",
       "   'events_url': 'https://api.github.com/users/CloseChoice/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/CloseChoice/received_events',\n",
       "   'type': 'User',\n",
       "   'user_view_type': 'public',\n",
       "   'site_admin': False},\n",
       "  'labels': [],\n",
       "  'state': 'open',\n",
       "  'locked': False,\n",
       "  'assignee': None,\n",
       "  'assignees': [],\n",
       "  'milestone': None,\n",
       "  'comments': 1,\n",
       "  'created_at': '2025-11-09T20:44:00Z',\n",
       "  'updated_at': '2025-11-10T16:40:50Z',\n",
       "  'closed_at': None,\n",
       "  'author_association': 'CONTRIBUTOR',\n",
       "  'type': None,\n",
       "  'active_lock_reason': None,\n",
       "  'draft': False,\n",
       "  'pull_request': {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/7858',\n",
       "   'html_url': 'https://github.com/huggingface/datasets/pull/7858',\n",
       "   'diff_url': 'https://github.com/huggingface/datasets/pull/7858.diff',\n",
       "   'patch_url': 'https://github.com/huggingface/datasets/pull/7858.patch',\n",
       "   'merged_at': None},\n",
       "  'body': 'This is PR builds on top of #7706 to revive the unfinished #6832 but isn\\'t just cleaning up things, here are some important changes:\\r\\n - `download_mode=\"FORCE_REDOWNLOAD\"` is interpreted as always creating a clean slate, that means that even if we already did:\\r\\n ```python\\r\\nload_dataset(\"<name>\")\\r\\nload_dataset(\"<name>\", split=\"train\", download_mode=\"force_redownload\")\\r\\n```\\r\\nThis makes sure that only the train dataset is available after executing both. This was different in the original PR, which proposed that train and test would be available. \\r\\n- `download_mode=\"REUSE_DATASET_IF_EXISTS\"` is interpreted as only ever adding new data, never redownloading OR deleting other splits. This was different in the original PR, where  \\r\\n ```python\\r\\nload_dataset(\"<name>\", split=\"test\")\\r\\nload_dataset(\"<name>\", split=\"train\")\\r\\n```\\r\\nresulted in only the train data being available, which I deem very unintuitive and probably not what users want. Also I argue that this is just the first step to a more user friendly partial loading when specifying percentages (or maybe even single instances) via the ReadInstructions, and then doing\\r\\n ```python\\r\\nload_dataset(\"<name>\", split=\"test[:10%]\")\\r\\nload_dataset(\"<name>\", split=\"test[10%:]\")\\r\\n```\\r\\nshould result IMO in the whole dataset being cached locally without redownloads.\\r\\n\\r\\nFurthermore this PR fixes a couple issues with the previous PR, e.g. a [missing comparison](https://github.com/huggingface/datasets/pull/7706/files#diff-f933ce41f71c6c0d1ce658e27de62cbe0b45d777e9e68056dd012ac3eb9324f7R877) and adding tests for the proposed changes in behaviour, which would both fail on @ArjunJagdale\\'s original PR. \\r\\n\\r\\nTodo:\\r\\n - [ ] update docs?\\r\\n\\r\\nFuture outlook (just my opinions and up for debate):\\r\\nAs mentioned before, I would see this as just a step towards the feature of partial percentage loading (though how the API should behave in that case is not entirely clear for me now) and maybe we could introduce another `download_mode=\"FORCE_REDOWNLOAD_SPLIT\"`, which makes sure that even if a split is specified, only the referenced split is redownloaded and everything else left unchanged, this would then allow users more granular control over what they want to redownload. \\r\\n@lhoestq very curious to get your opinion on this.',\n",
       "  'closed_by': None,\n",
       "  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/7858/reactions',\n",
       "   'total_count': 1,\n",
       "   '+1': 0,\n",
       "   '-1': 0,\n",
       "   'laugh': 0,\n",
       "   'hooray': 0,\n",
       "   'confused': 0,\n",
       "   'heart': 0,\n",
       "   'rocket': 1,\n",
       "   'eyes': 0},\n",
       "  'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/7858/timeline',\n",
       "  'performed_via_github_app': None,\n",
       "  'state_reason': None}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf738526-365f-41b3-9df5-650019dc3833",
   "metadata": {},
   "source": [
    "Create a .env file in the directory and out a single entry GITHUB_TOKEN=XXX in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd55bab4-4ef4-40f1-b7eb-a1e507a35a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e87ab904-4563-4ba1-8304-08c715bc3677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8796d79-87c5-4685-bf61-fcd1e49b3993",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Authorization\": f\"token {os.environ['GITHUB_TOKEN']}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa1a0a77-e824-40f6-90c9-f3a987e3b06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def fetch_issues(\n",
    "    owner=\"huggingface\",\n",
    "    repo=\"datasets\",\n",
    "    num_issues=10_000,\n",
    "    rate_limit=5_000,\n",
    "    issues_path=Path(\".\"),\n",
    "):\n",
    "    if not issues_path.is_dir():\n",
    "        issues_path.mkdir(exist_ok=True)\n",
    "\n",
    "    batch = []\n",
    "    all_issues = []\n",
    "    per_page = 100  # Number of issues to return per page\n",
    "    num_pages = math.ceil(num_issues / per_page)\n",
    "    base_url = \"https://api.github.com/repos\"\n",
    "\n",
    "    for page in tqdm(range(num_pages)):\n",
    "        # Query with state=all to get both open and closed issues\n",
    "        query = f\"issues?page={page}&per_page={per_page}&state=all\"\n",
    "        issues = requests.get(f\"{base_url}/{owner}/{repo}/{query}\", headers=headers)\n",
    "        batch.extend(issues.json())\n",
    "\n",
    "        if len(batch) > rate_limit and len(all_issues) < num_issues:\n",
    "            all_issues.extend(batch)\n",
    "            batch = []  # Flush batch for next time period\n",
    "            print(f\"Reached GitHub rate limit. Sleeping for one hour ...\")\n",
    "            time.sleep(60 * 60 + 1)\n",
    "\n",
    "    all_issues.extend(batch)\n",
    "    df = pd.DataFrame.from_records(all_issues)\n",
    "    df.to_json(f\"{issues_path}/{repo}-issues.jsonl\", orient=\"records\", lines=True)\n",
    "    print(\n",
    "        f\"Downloaded all the issues for {repo}! Dataset stored at {issues_path}/{repo}-issues.jsonl\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85e7e69b-efec-44c6-a34e-bf29ccade691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2aacc1ab3b741789758f9eb5849a8e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded all the issues for datasets! Dataset stored at ./datasets-issues.jsonl\n"
     ]
    }
   ],
   "source": [
    "fetch_issues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b61743f-59c8-4702-b549-4b04d871cacb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-llm-course--JTTk5sz-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
